{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project\n",
    "## Di-Tech Challenge: Forecasting Supply and Demand Gap for Didi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure to import the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta, time, date\n",
    "import re\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Files Overview\n",
    "The 600MB data files `training_set.tar.gz` and `test_set.tar.gz` can be downloaded at the [challenge page](http://research.xiaojukeji.com/competition/main.action?competitionId=DiTech2016)<sup>[1]</sup>.<br>\n",
    "After decompressing,<br> \n",
    "`/training_data/` folder contains the following directories and files (ignore the files being recognized as Unix executables):\n",
    "- `cluster_map` <img alt='cluster_map' src='../src/training_cluster_map.png' width='300'>\n",
    "- `poi_data` <img alt='poi_data' src='../src/training_poi_data.png' width='300'>\n",
    "- `order_data` <img alt='order_data' src='../src/training_order_data.png' width='300'>\n",
    "- `traffic_data` <img alt='traffic_data' src='../src/training_traffic_data.png' width='300'>\n",
    "- `weather_data` <img alt='weather_data' src='../src/training_weather_data.png' width='300'>\n",
    "\n",
    "`/test_set_2/` folder contains the following directories and files (ignore the files being recognized as Unix executables):\n",
    "<img alt='test_set_2' src='../src/test_set_2.png' width='300'>\n",
    "\n",
    "The target time slots for which gap values need to be predicted are provided in the `read_me_2.txt` in the format of `YYYY-MM-DD-time slot`.\n",
    "<img alt='read_me_2' src='../src/read_me_2.png' width='150'>\n",
    "\n",
    "The goal for the data processing is to load and combine all the data files to create a master dataset ready for model building.\n",
    "\n",
    "[1] _Update_: The current data files on the competition page are for final round. Due to the large size, if you want the first round data files we are using in this project, please contact me and I will send it to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of Cluster Map: (66, 2)\n"
     ]
    }
   ],
   "source": [
    "# load cluaster map data\n",
    "\n",
    "cluster_map_file = \"../raw_data/season_2/training_data/cluster_map/cluster_map\"\n",
    "cluster_map = pd.read_csv(cluster_map_file, delim_whitespace=True, header=None)\n",
    "cluster_map.columns = ['district_hash', 'district_id']\n",
    "\n",
    "print 'Dimension of Cluster Map: {}'.format(cluster_map.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of POI data: (66, 26)\n"
     ]
    }
   ],
   "source": [
    "# load poi data\n",
    "\n",
    "poi_data_file = \"../raw_data/season_2/training_data/poi_data/poi_data\"\n",
    "poi_data = pd.DataFrame(index=range(66), columns=['district_hash'] + ['poi_class' + str(i) for i in range(1, 26)])\n",
    "counter = 0\n",
    "\n",
    "with open(poi_data_file) as f:\n",
    "    while True:\n",
    "        poi_dict = {}\n",
    "        line = f.readline()\n",
    "        if line == '':\n",
    "            break\n",
    "        data = line.strip().split('\\t')\n",
    "        poi_dist_hash = data[0]\n",
    "        poi_dict['district_hash'] = poi_dist_hash\n",
    "        poi_log = data[1:]\n",
    "        for entry in poi_log:\n",
    "            entry = re.split(r'\\W', entry)\n",
    "            poi_class = entry[0]\n",
    "            poi_number = int(entry[-1])\n",
    "            key = 'poi_class' + poi_class\n",
    "            if key in poi_dict:\n",
    "                poi_dict[key] += poi_number\n",
    "            else:\n",
    "                poi_dict[key] = poi_number\n",
    "\n",
    "        poi_data.loc[counter] = pd.Series(poi_dict)\n",
    "        counter += 1\n",
    "\n",
    "poi_data.fillna(0, inplace=True)\n",
    "poi_data = pd.merge(poi_data, cluster_map, how='left', on='district_hash')\n",
    "poi_data.drop('district_hash', axis=1, inplace=True)\n",
    "poi_data.set_index('district_id', drop=False, inplace=True)\n",
    "poi_data.sort_values('district_id', inplace=True)\n",
    "\n",
    "print 'Dimension of POI data: {}'.format(poi_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_traffic_data(traffic_data_path):\n",
    "# load and preprocess traffic data\n",
    "\n",
    "    all_traffic_data_files = glob.glob(os.path.join(traffic_data_path, \"traffic_data*\"))\n",
    "    traffic_data = pd.DataFrame()\n",
    "    \n",
    "    for f in all_traffic_data_files:\n",
    "        df = pd.read_csv(f, delim_whitespace=True, header=None, names=['district_hash', 'level1_tj', \n",
    "                                                                       'level2_tj', 'level3_tj', 'level4_tj', \n",
    "                                                                       'date', 'timestamp'])\n",
    "\n",
    "        # create a new variable 'tj_level' = weighted sum of all levels of traffic\n",
    "        df['tj_level'] = (df['level1_tj'].apply(lambda x: int(x.split(':')[1]) * 0.1) + \\\n",
    "        df['level2_tj'].apply(lambda x: int(x.split(':')[1]) * 0.2) + \\\n",
    "        df['level3_tj'].apply(lambda x: int(x.split(':')[1]) * 0.3) + \\\n",
    "        df['level4_tj'].apply(lambda x: int(x.split(':')[1]) * 0.4)).map(int)\n",
    "\n",
    "        # convert date to weekday\n",
    "        df['weekday'] = df['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').weekday())\n",
    "\n",
    "        # convert timestamp to time slot \n",
    "        df_timestamp = df['timestamp'].apply(lambda x: datetime.strptime(x, '%H:%M:%S'))\n",
    "        df['time'] = df_timestamp.apply(lambda x: int((x - datetime(1900, 1, 1, 0, 0, 0)).total_seconds() \n",
    "                                                      / 60.0 / 10 + 1 ))\n",
    "\n",
    "        # create a new variable datetime that contains date and time in datetime type\n",
    "        df['datetime'] = df['date'] + ' ' + df['time'].apply(lambda x: str(timedelta(seconds=((int(x) - 1) * 600))))\n",
    "        df['datetime'] = df['datetime'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "        # convert district hash to district ID\n",
    "        df = df.merge(cluster_map, how='left', on='district_hash')\n",
    "\n",
    "        # create a new variable 'district_datetime' that combines each individual date time and district\n",
    "        df['district_datetime'] = df['district_id'].map(str) + ',' + df['date'] + '-' + df['time'].map(str)\n",
    "\n",
    "        # keep relevant columns\n",
    "        df.drop(['level1_tj', 'level2_tj', 'level3_tj', 'level4_tj', 'timestamp', 'district_hash'], \n",
    "                axis=1, inplace=True)\n",
    "\n",
    "        traffic_data = traffic_data.append(df, ignore_index=True)\n",
    "        traffic_data.sort_values(['district_id', 'datetime'], inplace=True)\n",
    "        traffic_data.set_index(traffic_data['district_datetime'], drop=False, inplace=True)\n",
    "\n",
    "    return traffic_data\n",
    "\n",
    "# load training and test traffic data\n",
    "training_traffic_data_path = \"../raw_data/season_2/training_data/traffic_data\"\n",
    "test_traffic_data_path = \"../raw_data/season_2/test_set_2/traffic_data\"\n",
    "\n",
    "training_traffic_data = load_traffic_data(training_traffic_data_path)\n",
    "test_traffic_data = load_traffic_data(test_traffic_data_path)\n",
    "\n",
    "# write out csv file\n",
    "training_traffic_data.to_csv('../clean_data/training_traffic_data.csv', index=False)\n",
    "test_traffic_data.to_csv('../clean_data/test_traffic_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_prev_tj(traffic_data):\n",
    "    # create new variables tj_prev1, tj_prev2 and tj_prev3 that record the tj_level value of last 1st, 2nd and\n",
    "    # 3rd time slot\n",
    "\n",
    "    traffic_data.sort_values(['district_id', 'datetime'], inplace=True)\n",
    "\n",
    "    traffic_data['tj_prev1'] = np.zeros(traffic_data.shape[0])\n",
    "    traffic_data['tj_prev2'] = np.zeros(traffic_data.shape[0])\n",
    "    traffic_data['tj_prev3'] = np.zeros(traffic_data.shape[0])\n",
    "\n",
    "    districts = traffic_data['district_id'].unique().tolist()\n",
    "    for i in districts:\n",
    "        indices = traffic_data[traffic_data['district_id']==i].index\n",
    "        tj_curr = traffic_data[traffic_data['district_id']==i]['tj_level'].values.tolist()\n",
    "        tj_prev1 = [tj_curr[0]] + tj_curr[:-1]\n",
    "\n",
    "        traffic_data.ix[traffic_data.district_id==i, 'tj_prev1'] = tj_prev1\n",
    "\n",
    "        tj_prev2 = [tj_prev1[0]] + tj_prev1[:-1]\n",
    "        traffic_data.ix[traffic_data.district_id==i, 'tj_prev2'] = tj_prev2\n",
    "\n",
    "        tj_prev3 = [tj_prev2[0]] + tj_prev2[:-1]\n",
    "        traffic_data.ix[traffic_data.district_id==i, 'tj_prev3'] = tj_prev3\n",
    "\n",
    "    return traffic_data\n",
    "\n",
    "training_traffic_data = get_prev_tj(training_traffic_data)\n",
    "test_traffic_data = get_prev_tj(test_traffic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_weather_data(weather_data_path):\n",
    "    # load and preprocess weather data\n",
    "    \n",
    "    all_weather_data_files = glob.glob(os.path.join(weather_data_path, \"weather_data*\"))\n",
    "\n",
    "    weather_data = pd.DataFrame()\n",
    "\n",
    "    for f in all_weather_data_files:\n",
    "        df = pd.read_csv(f, delim_whitespace=True, header=None, names = ['date', 'timestamp', 'weather', 'temp', 'pm'])\n",
    "\n",
    "        # convert timestamp to time slot\n",
    "        df_timestamp = df['timestamp'].apply(lambda x: datetime.strptime(x, '%H:%M:%S'))\n",
    "        df_time_slot = df_timestamp.apply(lambda x: int((x - datetime(1900, 1, 1, 0, 0, 0)).total_seconds() \n",
    "                                                        / 60.0 / 10 + 1 ))\n",
    "        df['time'] = df_time_slot.map(int)\n",
    "\n",
    "        # create a new variable datetime that contains date and time in datetime type\n",
    "        df['datetime'] = df['date'] + ' ' + df['time'].apply(lambda x: str(timedelta(seconds=((int(x) - 1) * 600))))\n",
    "        df['datetime'] = df['datetime'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "        # remove duplicated measurements for a time slot\n",
    "        df.drop_duplicates(subset='time', inplace=True)\n",
    "        \n",
    "        # keep relevant columns\n",
    "        df.drop('timestamp', axis=1, inplace=True)\n",
    "        \n",
    "        weather_data = weather_data.append(df,ignore_index=True)\n",
    "        weather_data.sort_values('datetime', inplace=True)\n",
    "        weather_data.set_index(weather_data['datetime'], drop=False, inplace=True)\n",
    "        \n",
    "\n",
    "    return weather_data\n",
    "\n",
    "\n",
    "# load training and test weather data\n",
    "training_weather_data_path = \"../raw_data/season_2/training_data/weather_data\"\n",
    "test_weather_data_path = \"../raw_data/season_2/test_set_2/weather_data\"\n",
    "\n",
    "training_weather_data = load_weather_data(training_weather_data_path)\n",
    "test_weather_data = load_weather_data(test_weather_data_path)\n",
    "\n",
    "# write out csv file\n",
    "training_weather_data.to_csv('../clean_data/training_weather_data.csv', index=False)\n",
    "test_weather_data.to_csv('../clean_data/test_weather_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# laod order data, takes approximately 10 minutes\n",
    "\n",
    "def load_order_data(order_data_path):\n",
    "    # load order data\n",
    "    \n",
    "    all_order_data_files = glob.glob(os.path.join(order_data_path, \"order_data*\"))\n",
    "    order_data = pd.DataFrame()\n",
    "\n",
    "    for f in all_order_data_files:\n",
    "        df = pd.read_csv(f, delim_whitespace=True, header=None, names=['order_id', 'driver_id', 'passenger_id', \n",
    "                                                                       'district_hash', 'dest_district_hash',\n",
    "                                                                       'price', 'date', 'timestamp'])\n",
    "        # remove duplicated orders\n",
    "        df.drop_duplicates(subset='order_id', inplace=True)\n",
    "        \n",
    "        df.drop(['order_id', 'passenger_id', 'dest_district_hash', 'price'], axis=1, inplace=True)\n",
    "        \n",
    "        # convert date to weekday\n",
    "        df['weekday'] = df['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').weekday())\n",
    "\n",
    "        # convert timestamp to time slot \n",
    "        df_timestamp = df['timestamp'].apply(lambda x: datetime.strptime(x, '%H:%M:%S'))\n",
    "        df['time'] = df_timestamp.apply(lambda x: int((x - datetime(1900, 1, 1, 0, 0, 0)).total_seconds() \n",
    "                                                      / 60.0 / 10 + 1 ))\n",
    "\n",
    "        # create a new variable datetime that contains date and time in datetime type\n",
    "        df['datetime'] = df['date'] + ' ' + df['time'].apply(lambda x: str(timedelta(seconds=((int(x) - 1) * 600))))\n",
    "        df['datetime'] = df['datetime'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "        # convert district hash to district ID\n",
    "        df = df.merge(cluster_map, how='left', on='district_hash')\n",
    "\n",
    "        # create a new variable 'district_datetime' that combines each individual date time and district\n",
    "        df['district_datetime'] = df['district_id'].map(str) + ',' + df['date'] + '-' + df['time'].map(str)\n",
    "\n",
    "\n",
    "        # keep relevant columns\n",
    "        df.drop(['district_hash', 'timestamp'], axis=1, inplace=True)\n",
    "\n",
    "        order_data = order_data.append(df, ignore_index=True)\n",
    "        order_data.sort_values(['district_id', 'datetime'], inplace=True)\n",
    "        order_data.set_index(order_data['district_datetime'], drop=False, inplace=True)\n",
    "\n",
    "    return order_data\n",
    "\n",
    "\n",
    "# load training and test weather data\n",
    "training_order_data_path = \"../raw_data/season_2/training_data/order_data\"\n",
    "test_order_data_path = \"../raw_data/season_2/test_set_2/order_data\"\n",
    "\n",
    "training_order_data = load_order_data(training_order_data_path)\n",
    "test_order_data = load_order_data(test_order_data_path)\n",
    "\n",
    "# write out csv file\n",
    "training_order_data.to_csv('../clean_data/training_order_data.csv', index=False)\n",
    "test_order_data.to_csv('../clean_data/test_order_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_gap(order_data):\n",
    "    # count NaN driver_id's\n",
    "    \n",
    "    gap = pd.DataFrame(order_data.groupby(['district_datetime'])['driver_id'].apply(lambda x: x.isnull().sum()))\n",
    "    gap.columns = ['gap']\n",
    "    gap['district_datetime'] = gap.index\n",
    "    return gap\n",
    "\n",
    "training_gap = get_gap(training_order_data)\n",
    "test_gap = get_gap(test_order_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_data(poi_data, traffic_data, weather_data, gap):\n",
    "    # merge all datasets\n",
    "    \n",
    "    all_data = pd.DataFrame(columns=['district_datetime', 'district_id', 'datetime', \n",
    "                                     'date', 'time', 'weekday'])\n",
    "    all_data['district_datetime'] = gap['district_datetime']\n",
    "    all_data = all_data.merge(gap, how='outer', on='district_datetime')\n",
    "    all_data['district_id'] = all_data['district_datetime'].apply(lambda x: int(x.split(',')[0]))\n",
    "    all_data['datetime'] = all_data['district_datetime'].apply(lambda x: x.split(',')[1])\n",
    "    all_data['date'] = all_data['datetime'].apply(lambda x: '-'.join(x.split('-')[0:3]))\n",
    "    all_data['weekday'] = all_data['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d').weekday())\n",
    "    all_data['time'] = all_data['datetime'].apply(lambda x: int(x.split('-')[-1]))\n",
    "    all_data['datetime'] = all_data['date'] + ' ' + all_data['time'].apply(\n",
    "        lambda x: str(timedelta(seconds=((int(x) - 1) * 600))))\n",
    "    all_data['datetime'] = all_data['datetime'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    all_data.sort_values(['district_id', 'datetime'], inplace=True)\n",
    "\n",
    "    # create new variables gap_prev1, gap_prev2 and gap_prev3 that record the gap value of last 1st, 2nd and\n",
    "    # 3rd time slot\n",
    "    all_data['gap_prev1'] = np.zeros(all_data.shape[0])\n",
    "    all_data['gap_prev2'] = np.zeros(all_data.shape[0])\n",
    "    all_data['gap_prev3'] = np.zeros(all_data.shape[0])\n",
    "    \n",
    "    districts = all_data['district_id'].unique().tolist()\n",
    "    for i in districts:\n",
    "        gap_curr = all_data[all_data['district_id']==i]['gap'].values.tolist()\n",
    "        gap_prev1 = [gap_curr[0]] + gap_curr[:-1]\n",
    "        all_data.ix[all_data.district_id==i, 'gap_prev1'] = gap_prev1\n",
    "\n",
    "        gap_prev2 = [gap_prev1[0]] + gap_prev1[:-1]\n",
    "        all_data.ix[all_data.district_id==i, 'gap_prev2'] = gap_prev2\n",
    "\n",
    "        gap_prev3 = [gap_prev2[0]] + gap_prev2[:-1]\n",
    "        all_data.ix[all_data.district_id==i, 'gap_prev3'] = gap_prev3\n",
    "    \n",
    "\n",
    "    all_data = all_data.merge(poi_data, how='left', on='district_id')\n",
    "    all_data = all_data.merge(traffic_data[['district_datetime', 'tj_level', 'tj_prev1', 'tj_prev2', 'tj_prev3']], \n",
    "                              how='left', on='district_datetime')   \n",
    "    all_data = all_data.merge(weather_data[['datetime', 'weather', 'temp', 'pm']], how='left', on='datetime')\n",
    "    \n",
    "    all_data.drop('date', axis=1, inplace=True)\n",
    "    all_data.sort_values(['district_id', 'datetime'], inplace=True)\n",
    "    all_data.set_index('datetime', drop=False, inplace=True)\n",
    "\n",
    "    # fill in missing weather data with interpolation\n",
    "    weather_feature_list = ['weather', 'temp', 'pm']\n",
    "    for feature in weather_feature_list:\n",
    "        all_data[feature] = all_data[feature].interpolate(method='time').apply(np.round)\n",
    "    \n",
    "    # fill in missing traffic jam level\n",
    "    traffic_feature_list = ['tj_level', 'tj_prev1', 'tj_prev2', 'tj_prev3']\n",
    "    for feature in traffic_feature_list:\n",
    "        all_data[feature] = all_data[feature].fillna(method='bfill')\n",
    "        all_data[feature] = all_data[feature].fillna(method='ffill')\n",
    "    \n",
    "    all_data.sort_values(['datetime', 'district_id'], inplace=True)\n",
    "    all_data.set_index('datetime', drop=False, inplace=True)\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "training_all_data = get_all_data(poi_data, training_traffic_data, training_weather_data, training_gap)\n",
    "test_all_data = get_all_data(poi_data, test_traffic_data, test_weather_data, test_gap)\n",
    "\n",
    "# write out csv file\n",
    "training_all_data.to_csv('../clean_data/training_all_data.csv', index=False)\n",
    "test_all_data.to_csv('../clean_data/test_all_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load targets to be predicted\n",
    "\n",
    "target = pd.read_csv(\"../raw_data/season_2/test_set_2/read_me_2.txt\", header=None, skiprows=[0], names=['datetime'])\n",
    "n = target.shape[0]\n",
    "\n",
    "districts = np.repeat(np.array(range(1, 67)), n)\n",
    "                 \n",
    "target = pd.concat([target]*66, ignore_index=True)\n",
    "target['district_id'] = pd.Series(districts)\n",
    "target['date'] = target['datetime'].apply(lambda x: '-'.join(x.split('-')[0:3]))\n",
    "target['weekday'] = target['datetime'].apply(lambda x: datetime.strptime('-'.join(x.split('-')[0:3]), \n",
    "                                                                         '%Y-%m-%d').weekday())\n",
    "target['time'] = target['datetime'].apply(lambda x: x.split('-')[-1])\n",
    "target['datetime'] = target['date'] + ' ' + target['time'].apply(\n",
    "    lambda x: str(timedelta(seconds=((int(x) - 1) * 600))))\n",
    "target['datetime'] = target['datetime'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "target['district_datetime'] = target['district_id'].map(str) + ',' + target['date'] + '-' + target['time'].map(str)\n",
    "\n",
    "target.to_csv('../clean_data/target.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":D DATA ARE READY FOR DATA ANALYSIS AND MODEL BUILDING :D\n"
     ]
    }
   ],
   "source": [
    "# get test set by combining target and test data \n",
    "\n",
    "target_set = pd.merge(test_all_data[['district_id', 'datetime', 'district_datetime', 'weekday', 'time', 'weather', \n",
    "                                   'temp', 'pm', 'gap', 'gap_prev1', 'gap_prev2', 'gap_prev3', 'tj_level', 'tj_prev1', \n",
    "                                   'tj_prev2', 'tj_prev3']], \n",
    "                    target.drop('date', axis=1, inplace=False), how='outer', \n",
    "                    on=['district_id', 'datetime', 'district_datetime', 'weekday', 'time'])\n",
    "\n",
    "target_set = target_set.merge(poi_data, how='outer', on='district_id')\n",
    "\n",
    "target_set.sort_values(['district_id', 'datetime'], inplace=True)\n",
    "target_set.set_index('datetime', drop=False, inplace=True)\n",
    "\n",
    "# fill in missing weather data with interpolation\n",
    "weather_feature_list = ['weather', 'temp', 'pm']\n",
    "for feature in weather_feature_list:\n",
    "    target_set[feature] = target_set[feature].interpolate(method='time').apply(np.round)\n",
    "    \n",
    "# fill in missing traffic jam level\n",
    "traffic_feature_list = ['tj_level', 'tj_prev1', 'tj_prev2', 'tj_prev3']\n",
    "\n",
    "for feature in traffic_feature_list:\n",
    "    target_set[feature] = target_set[feature].fillna(method='bfill')\n",
    "    target_set[feature] = target_set[feature].fillna(method='ffill')\n",
    "    \n",
    "target_set.to_csv('../clean_data/target_set.csv', index=False)\n",
    "\n",
    "print \":D DATA ARE READY FOR DATA ANALYSIS AND MODEL BUILDING :D\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
